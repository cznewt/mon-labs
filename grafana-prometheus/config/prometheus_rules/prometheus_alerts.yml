{
   "groups": [
      {
         "name": "prometheus",
         "rules": [
            {
               "alert": "PrometheusBadConfig",
               "annotations": {
                  "description": "Prometheus {{$labels.instance}} has failed to reload its configuration.",
                  "summary": "Failed Prometheus configuration reload."
               },
               "expr": "# Without max_over_time, failed scrapes could create false negatives, see\n# https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.\nmax_over_time(prometheus_config_last_reload_successful{job=\"prometheus\"}[5m]) == 0\n",
               "for": "10m",
               "labels": {
                  "severity": "critical"
               }
            },
            {
               "alert": "PrometheusNotificationQueueRunningFull",
               "annotations": {
                  "description": "Alert notification queue of Prometheus {{$labels.instance}} is running full.",
                  "summary": "Prometheus alert notification queue predicted to run full in less than 30m."
               },
               "expr": "# Without min_over_time, failed scrapes could create false negatives, see\n# https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.\n(\n  predict_linear(prometheus_notifications_queue_length{job=\"prometheus\"}[5m], 60 * 30)\n>\n  min_over_time(prometheus_notifications_queue_capacity{job=\"prometheus\"}[5m])\n)\n",
               "for": "15m",
               "labels": {
                  "severity": "warning"
               }
            },
            {
               "alert": "PrometheusErrorSendingAlertsToSomeAlertmanagers",
               "annotations": {
                  "description": "{{ printf \"%.1f\" $value }}% errors while sending alerts from Prometheus {{$labels.instance}} to Alertmanager {{$labels.alertmanager}}.",
                  "summary": "Prometheus has encountered more than 1% errors sending alerts to a specific Alertmanager."
               },
               "expr": "(\n  rate(prometheus_notifications_errors_total{job=\"prometheus\"}[5m])\n/\n  rate(prometheus_notifications_sent_total{job=\"prometheus\"}[5m])\n)\n* 100\n> 1\n",
               "for": "15m",
               "labels": {
                  "severity": "warning"
               }
            },
            {
               "alert": "PrometheusErrorSendingAlertsToAnyAlertmanager",
               "annotations": {
                  "description": "{{ printf \"%.1f\" $value }}% minimum errors while sending alerts from Prometheus {{$labels.instance}} to any Alertmanager.",
                  "summary": "Prometheus encounters more than 3% errors sending alerts to any Alertmanager."
               },
               "expr": "min without(alertmanager) (\n  rate(prometheus_notifications_errors_total{job=\"prometheus\"}[5m])\n/\n  rate(prometheus_notifications_sent_total{job=\"prometheus\"}[5m])\n)\n* 100\n> 3\n",
               "for": "15m",
               "labels": {
                  "severity": "critical"
               }
            },
            {
               "alert": "PrometheusNotConnectedToAlertmanagers",
               "annotations": {
                  "description": "Prometheus {{$labels.instance}} is not connected to any Alertmanagers.",
                  "summary": "Prometheus is not connected to any Alertmanagers."
               },
               "expr": "# Without max_over_time, failed scrapes could create false negatives, see\n# https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.\nmax_over_time(prometheus_notifications_alertmanagers_discovered{job=\"prometheus\"}[5m]) < 1\n",
               "for": "10m",
               "labels": {
                  "severity": "warning"
               }
            },
            {
               "alert": "PrometheusTSDBReloadsFailing",
               "annotations": {
                  "description": "Prometheus {{$labels.instance}} has detected {{$value | humanize}} reload failures over the last 3h.",
                  "summary": "Prometheus has issues reloading blocks from disk."
               },
               "expr": "increase(prometheus_tsdb_reloads_failures_total{job=\"prometheus\"}[3h]) > 0\n",
               "for": "4h",
               "labels": {
                  "severity": "warning"
               }
            },
            {
               "alert": "PrometheusTSDBCompactionsFailing",
               "annotations": {
                  "description": "Prometheus {{$labels.instance}} has detected {{$value | humanize}} compaction failures over the last 3h.",
                  "summary": "Prometheus has issues compacting blocks."
               },
               "expr": "increase(prometheus_tsdb_compactions_failed_total{job=\"prometheus\"}[3h]) > 0\n",
               "for": "4h",
               "labels": {
                  "severity": "warning"
               }
            },
            {
               "alert": "PrometheusNotIngestingSamples",
               "annotations": {
                  "description": "Prometheus {{$labels.instance}} is not ingesting samples.",
                  "summary": "Prometheus is not ingesting samples."
               },
               "expr": "rate(prometheus_tsdb_head_samples_appended_total{job=\"prometheus\"}[5m]) <= 0\n",
               "for": "10m",
               "labels": {
                  "severity": "warning"
               }
            },
            {
               "alert": "PrometheusDuplicateTimestamps",
               "annotations": {
                  "description": "Prometheus {{$labels.instance}} is dropping {{ printf \"%.4g\" $value  }} samples/s with different values but duplicated timestamp.",
                  "summary": "Prometheus is dropping samples with duplicate timestamps."
               },
               "expr": "rate(prometheus_target_scrapes_sample_duplicate_timestamp_total{job=\"prometheus\"}[5m]) > 0\n",
               "for": "10m",
               "labels": {
                  "severity": "warning"
               }
            },
            {
               "alert": "PrometheusOutOfOrderTimestamps",
               "annotations": {
                  "description": "Prometheus {{$labels.instance}} is dropping {{ printf \"%.4g\" $value  }} samples/s with timestamps arriving out of order.",
                  "summary": "Prometheus drops samples with out-of-order timestamps."
               },
               "expr": "rate(prometheus_target_scrapes_sample_out_of_order_total{job=\"prometheus\"}[5m]) > 0\n",
               "for": "10m",
               "labels": {
                  "severity": "warning"
               }
            },
            {
               "alert": "PrometheusRemoteStorageFailures",
               "annotations": {
                  "description": "Prometheus {{$labels.instance}} failed to send {{ printf \"%.1f\" $value }}% of the samples to queue {{$labels.queue}}.",
                  "summary": "Prometheus fails to send samples to remote storage."
               },
               "expr": "(\n  rate(prometheus_remote_storage_failed_samples_total{job=\"prometheus\"}[5m])\n/\n  (\n    rate(prometheus_remote_storage_failed_samples_total{job=\"prometheus\"}[5m])\n  +\n    rate(prometheus_remote_storage_succeeded_samples_total{job=\"prometheus\"}[5m])\n  )\n)\n* 100\n> 1\n",
               "for": "15m",
               "labels": {
                  "severity": "critical"
               }
            },
            {
               "alert": "PrometheusRemoteWriteBehind",
               "annotations": {
                  "description": "Prometheus {{$labels.instance}} remote write is {{ printf \"%.1f\" $value }}s behind for queue {{$labels.queue}}.",
                  "summary": "Prometheus remote write is behind."
               },
               "expr": "# Without max_over_time, failed scrapes could create false negatives, see\n# https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.\n(\n  max_over_time(prometheus_remote_storage_highest_timestamp_in_seconds{job=\"prometheus\"}[5m])\n- on(job, instance) group_right\n  max_over_time(prometheus_remote_storage_queue_highest_sent_timestamp_seconds{job=\"prometheus\"}[5m])\n)\n> 120\n",
               "for": "15m",
               "labels": {
                  "severity": "critical"
               }
            },
            {
               "alert": "PrometheusRemoteWriteDesiredShards",
               "annotations": {
                  "description": "Prometheus {{$labels.instance}} remote write desired shards calculation wants to run {{ $value }} shards, which is more than the max of {{ printf `prometheus_remote_storage_shards_max{instance=\"%s\",job=\"prometheus\"}` $labels.instance | query | first | value }}.",
                  "summary": "Prometheus remote write desired shards calculation wants to run more than configured max shards."
               },
               "expr": "# Without max_over_time, failed scrapes could create false negatives, see\n# https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.\n(\n  max_over_time(prometheus_remote_storage_shards_desired{job=\"prometheus\"}[5m])\n>\n  max_over_time(prometheus_remote_storage_shards_max{job=\"prometheus\"}[5m])\n)\n",
               "for": "15m",
               "labels": {
                  "severity": "warning"
               }
            },
            {
               "alert": "PrometheusRuleFailures",
               "annotations": {
                  "description": "Prometheus {{$labels.instance}} has failed to evaluate {{ printf \"%.0f\" $value }} rules in the last 5m.",
                  "summary": "Prometheus is failing rule evaluations."
               },
               "expr": "increase(prometheus_rule_evaluation_failures_total{job=\"prometheus\"}[5m]) > 0\n",
               "for": "15m",
               "labels": {
                  "severity": "critical"
               }
            },
            {
               "alert": "PrometheusMissingRuleEvaluations",
               "annotations": {
                  "description": "Prometheus {{$labels.instance}} has missed {{ printf \"%.0f\" $value }} rule group evaluations in the last 5m.",
                  "summary": "Prometheus is missing rule evaluations due to slow rule group evaluation."
               },
               "expr": "increase(prometheus_rule_group_iterations_missed_total{job=\"prometheus\"}[5m]) > 0\n",
               "for": "15m",
               "labels": {
                  "severity": "warning"
               }
            }
         ]
      }
   ]
}
